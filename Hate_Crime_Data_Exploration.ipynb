{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/Analyzing-Hate-Crime-Data/blob/main/Hate_Crime_Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOiwCxBSfrVr"
      },
      "source": [
        "#TODO:\n",
        "####Fix multicol parsing so that:\n",
        "*   Dummy column values are set correctly (so that they're not all 0's) (resetting the index might've fixed this)\n",
        "*   The correct columns are being parsed (not 85 years and over or whatever is causing that)\n",
        "*   A secret third thing not listed here (still gotta come up with this one)\n",
        "\n",
        "####Include offense_name and location_name in crime_df categorical data once you've refactored multicol parsing\n",
        "####Add the ethnicity_race_cols to demo_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1drJWUkF4odQ"
      },
      "source": [
        "#Import Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXLq0TJ6dd75"
      },
      "source": [
        "###Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YVKUvdikdVxP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH0vaj3TdsQ_"
      },
      "source": [
        "###Import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bCX3rTyyddMq"
      },
      "outputs": [],
      "source": [
        "crime_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/hate_crime/hate_crime.csv\", on_bad_lines='skip')\n",
        "city_demo_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/city/ACSST1Y2022.S0101-Data.csv\", on_bad_lines='skip')\n",
        "county_demo_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/county/ACSDP1Y2022.DP05-Data.csv\", on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMmPAMsD4kTu"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n5X6ugA1wk7E"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ad1nsDl75WNg"
      },
      "outputs": [],
      "source": [
        "#only consider 2022 crime data from cities and counties\n",
        "crime_df = crime_df[(crime_df['data_year'] == 2022) & ((crime_df['agency_type_name'] == \"City\") | (crime_df['agency_type_name'] == \"County\"))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_ky8rB7u13"
      },
      "source": [
        "##Clean city and county demographic datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MvzdSWFyzeW_"
      },
      "outputs": [],
      "source": [
        "# use the entries of the first row as the column headers for easier management\n",
        "def fixHeaders(df):\n",
        "  new_headers = df.iloc[0]\n",
        "  new_df = df[1:]\n",
        "  new_df.columns = new_headers\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9WA3B15xen01"
      },
      "outputs": [],
      "source": [
        "#fix the headers\n",
        "city_demo_df = fixHeaders(city_demo_df)\n",
        "county_demo_df = fixHeaders(county_demo_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create area columns"
      ],
      "metadata": {
        "id": "oYwpwcA8aqsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wptIjBBS43De"
      },
      "outputs": [],
      "source": [
        "def split_area_name(area):\n",
        "    result = [np.nan, np.nan, np.nan]\n",
        "    if \", \" in area:\n",
        "        result = area.split(\", \") + [np.nan]\n",
        "    if \" city\" in result[0].lower():\n",
        "        result[0] = result[0][0:-5]\n",
        "        result[2] = result[1]\n",
        "        result[1] = \"City\"\n",
        "    elif \" county\" in result[0].lower():\n",
        "        result[0] = result[0][0:-7]\n",
        "        result[2] = result[1]\n",
        "        result[1] = \"County\"\n",
        "\n",
        "    return result + [np.nan] * (3 - len(result))\n",
        "\n",
        "def splitArea(df):\n",
        "    df[[\"pug_agency_name\", \"agency_type_name\", \"state_name\"]] = df[\"Geographic Area Name\"].map(split_area_name).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vDxDbqFs5ax-"
      },
      "outputs": [],
      "source": [
        "splitArea(city_demo_df)\n",
        "splitArea(county_demo_df)\n",
        "\n",
        "# city_demo_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zouCbvk6_ccD"
      },
      "source": [
        "###Fix column names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZfZR5Bv1OalM"
      },
      "outputs": [],
      "source": [
        "def fix_column_names(df):\n",
        "  removalList = {\"SEX AND AGE!!\", \"SEX AND \", \"Estimate!!\", \"Total!!\", \"Total population!!\", \"CITIZEN, VOTING AGE POPULATION!!\", \"AGE!!\"}\n",
        "\n",
        "  new_cols = []\n",
        "  for col in df.columns:\n",
        "    col = str(col)\n",
        "    for term in removalList:\n",
        "      if term in col:\n",
        "        col = col.replace(term, \"\")\n",
        "    new_cols.append(col)\n",
        "\n",
        "  return new_cols\n",
        "\n",
        "county_demo_df.columns = fix_column_names(county_demo_df)\n",
        "city_demo_df.columns = fix_column_names(city_demo_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Add and remove columns to line the dataframes up"
      ],
      "metadata": {
        "id": "Ns7wjEW0bLTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oLuvQpJzkLlT"
      },
      "outputs": [],
      "source": [
        "#combine the city_df age-related columns as needed to merge better with the county_demo_df\n",
        "def combine_city_df_cols(df_s0101):\n",
        "    formatted_df = df_s0101.copy(deep=True)\n",
        "\n",
        "    formatted_df['25 to 34 years'] = df_s0101['25 to 29 years'] + df_s0101['30 to 34 years']\n",
        "    formatted_df['35 to 44 years'] = df_s0101['35 to 39 years'] + df_s0101['40 to 44 years']\n",
        "    formatted_df['45 to 54 years'] = df_s0101['45 to 49 years'] + df_s0101['50 to 54 years']\n",
        "    formatted_df['65 to 74 years'] = df_s0101['65 to 69 years'] + df_s0101['70 to 74 years']\n",
        "    formatted_df['75 to 84 years'] = df_s0101['75 to 79 years'] + df_s0101['80 to 84 years']\n",
        "\n",
        "    return formatted_df\n",
        "\n",
        "city_demo_df = combine_city_df_cols(city_demo_df)\n",
        "# city_demo_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yUk3NyAxVAc2"
      },
      "outputs": [],
      "source": [
        "# fix any missing values\n",
        "city_demo_df = city_demo_df.replace(\"(X)\", np.nan)\n",
        "county_demo_df = county_demo_df.replace(\"(X)\", np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop all margin of error columns from the city and county dataframes\n",
        "def drop_cols_containing(df, pattern):\n",
        "    df.drop(columns=list(df.filter(regex = pattern)), inplace = True)\n",
        "\n",
        "drop_cols_containing(city_demo_df, \"Margin of Error|SUMMARY INDICATORS|PERCENT ALLOCATED|SELECTED AGE CATEGORIES|Female\")\n",
        "drop_cols_containing(county_demo_df, \"Margin of Error|SUMMARY INDICATORS|PERCENT ALLOCATED\")\n",
        "\n",
        "# city_demo_df.dtypes"
      ],
      "metadata": {
        "id": "G7cTb7FkY-Rl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fix column data types"
      ],
      "metadata": {
        "id": "8hlE0Oe8bUbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = county_demo_df.columns.value_counts()\n",
        "mask = counts > 1\n",
        "duplicates = list(counts[mask].index)\n",
        "county_demo_df[duplicates].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALM0fmMBEJAC",
        "outputId": "5f2927fc-9390-464a-fd84-3c4c275b2380"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65 years and over                   object\n",
              "65 years and over                   object\n",
              "18 years and over                   object\n",
              "18 years and over                   object\n",
              "Percent!!65 years and over          object\n",
              "Percent!!65 years and over          object\n",
              "Percent!!RACE!!One race             object\n",
              "Percent!!RACE!!One race             object\n",
              "Percent!!RACE!!Two or More Races    object\n",
              "Percent!!RACE!!Two or More Races    object\n",
              "Percent!!18 years and over          object\n",
              "Percent!!18 years and over          object\n",
              "RACE!!One race                      object\n",
              "RACE!!One race                      object\n",
              "RACE!!Two or More Races             object\n",
              "RACE!!Two or More Races             object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df = county_demo_df.loc[:, ~county_demo_df.columns.duplicated(keep='last')]"
      ],
      "metadata": {
        "id": "fAyrAm9yFOvt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_percent_age_cols = list(set(list(city_demo_df.filter(regex = \"Percent\")) + list(city_demo_df.filter(regex = \"ratio\")) + list(city_demo_df.filter(regex = \"years\"))))\n",
        "county_percent_age_cols = list(set(list(county_demo_df.filter(regex = \"Percent\")) + list(county_demo_df.filter(regex = \"ratio\")) + list(county_demo_df.filter(regex = \"years\"))))\n",
        "\n",
        "for column in city_percent_age_cols:\n",
        "    city_demo_df = city_demo_df[~city_demo_df[column].astype(str).str.contains('N')]\n",
        "\n",
        "for column in county_percent_age_cols:\n",
        "    county_demo_df = county_demo_df[~county_demo_df[column].astype(str).str.contains('N')]"
      ],
      "metadata": {
        "id": "jZYIR6d-iPJR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_demo_df[city_percent_age_cols] = city_demo_df[city_percent_age_cols].astype(float)\n",
        "county_demo_df[county_percent_age_cols] = county_demo_df[county_percent_age_cols].astype(float)"
      ],
      "metadata": {
        "id": "ZrdOlh7lGDt9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the quantitative columns to int types\n",
        "# continuous quantitative variables\n",
        "continuous_int_columns = [\"Total population\", \"Under 5 years\", \"5 to 9 years\", \"45 to 54 years\", \"10 to 14 years\", \"75 to 84 years\", \"60 to 64 years\",\\\n",
        "                      \"25 to 34 years\", \"15 to 19 years\", \"20 to 24 years\", \"35 to 44 years\", \"55 to 59 years\", \"65 to 74 years\",\\\n",
        "                      \"85 years and over\"]# + list(city_demo_df.filter(regex = \"Total population\"))))\n",
        "\n",
        "for column in continuous_int_columns:\n",
        "    city_demo_df = city_demo_df[~city_demo_df[column].astype(str).str.contains('N')]\n",
        "    county_demo_df = county_demo_df[~county_demo_df[column].astype(str).str.contains('N')]\n",
        "\n",
        "city_demo_df[continuous_int_columns] = city_demo_df[continuous_int_columns].astype(int)\n",
        "county_demo_df[continuous_int_columns] = county_demo_df[continuous_int_columns].astype(int)"
      ],
      "metadata": {
        "id": "Q8BIgfCuZ3cS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_groups = ['Under 5 years', '5 to 9 years', '10 to 14 years', '15 to 19 years', '20 to 24 years', '25 to 34 years', '35 to 44 years', '45 to 54 years', '55 to 59 years', '60 to 64 years', '65 to 74 years', '75 to 84 years', '85 years and over']"
      ],
      "metadata": {
        "id": "fdq9xxRHsFiM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Drop more invalid data"
      ],
      "metadata": {
        "id": "yYKBzufDuKl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# city_demo_df.head()"
      ],
      "metadata": {
        "id": "q4ug9w0EwJ4z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for group in age_groups:\n",
        "#     city_demo_df = city_demo_df[city_demo_df[group] < 10**7]\n",
        "#     county_demo_df = county_demo_df[county_demo_df[group] < 10**7]\n",
        "\n",
        "# city_demo_df.head()"
      ],
      "metadata": {
        "id": "fmQBOYl5tgxM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxkvqx0R8A29"
      },
      "source": [
        "##Clean crime dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jVRTkrMofxu"
      },
      "source": [
        "###Break up incident date information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LWs_l73xYNHa"
      },
      "outputs": [],
      "source": [
        "def split_incident_date(date):\n",
        "    result = date.split(\"-\")\n",
        "\n",
        "    return result + [np.nan] * (3 - len(result))\n",
        "\n",
        "crime_df[[\"data_year\", \"incident_month\", \"incident_day\"]] = crime_df[\"incident_date\"].map(split_incident_date).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Break up crime_df categorical columns containing multiple categories into dummies"
      ],
      "metadata": {
        "id": "8xOo7lDlSHDU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgKZ6LO0rur"
      },
      "source": [
        "####Important functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "whQdcLBmrJus"
      },
      "outputs": [],
      "source": [
        "def get_max_cols(df, column, delimiter):\n",
        "    max = 0\n",
        "\n",
        "    # for each entry\n",
        "    for i in df.index:\n",
        "      val = df.loc[i, column]\n",
        "\n",
        "      if val != np.nan and type(val) != float:\n",
        "        # try:\n",
        "        temp = len(str(val).split(delimiter))\n",
        "        # except:\n",
        "        #     print(val)\n",
        "\n",
        "        if temp > max:\n",
        "          max = temp\n",
        "          # print(max)\n",
        "\n",
        "      else:\n",
        "        print(\"val was np.nan or float; unable to parse\")\n",
        "\n",
        "      return max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_type_cols_map(type_prefix, max_type_cols):\n",
        "  type_cols_map = {}\n",
        "  for i in range(max_type_cols):\n",
        "    type_cols_map[i] = type_prefix + str(i)\n",
        "\n",
        "  return type_cols_map\n"
      ],
      "metadata": {
        "id": "TTboZ_ntkpam"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d-HY4ZyLA4nt"
      },
      "outputs": [],
      "source": [
        "max_cols = 0\n",
        "\n",
        "def split_types(val):\n",
        "  # break up the col_vals into individual col_vals\n",
        "  col_vals = str(val).split(\";\")\n",
        "  for i in range(len(col_vals)):\n",
        "      col_vals[i] = col_vals[i].strip()\n",
        "\n",
        "  # add placeholders\n",
        "  col_vals = col_vals + [np.nan] * (max_cols - len(col_vals))\n",
        "\n",
        "  return col_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Y5Xs8UBRqRtP"
      },
      "outputs": [],
      "source": [
        "def split_multiCol(df, column, replacementDict, multicol_cols, max_columns):\n",
        "    for i in df.index:\n",
        "      if type(df.loc[i, column]) == str:\n",
        "        # and for each replacement to be made\n",
        "        for before, after in replacementDict.items():\n",
        "          df.loc[i, column] = df.loc[i, column].replace(before, after)\n",
        "        # remove extra whitespace\n",
        "        df.loc[i, column].strip()\n",
        "\n",
        "    # change max_cols here, since you can't do it from the mapping function\n",
        "    max_cols = max_columns\n",
        "\n",
        "    multicol_df = df[column].map(split_types).apply(pd.Series)\n",
        "    multicol_df.rename(columns=multicol_cols, inplace=True)\n",
        "\n",
        "    # remove the np.nan column, which I'm not entirely sure why exists\n",
        "    if np.nan in multicol_df.columns:\n",
        "      multicol_df = multicol_df.drop(columns=[np.nan])\n",
        "\n",
        "    return multicol_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UtuXXR3g0nwq"
      },
      "outputs": [],
      "source": [
        "def get_unique_types(df, type_cols):\n",
        "  all_types = []\n",
        "  for col in type_cols:\n",
        "    all_types.extend(df[col].unique())\n",
        "\n",
        "  # create a set of just the unique ones\n",
        "  unique_types = set(all_types)\n",
        "\n",
        "  # remove np.nan\n",
        "  unique_types.remove(np.nan)\n",
        "\n",
        "  return unique_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SdJyFKJfzdHr"
      },
      "outputs": [],
      "source": [
        "def get_unique_dummies(df, unique_types):\n",
        "  unique_dummies = pd.get_dummies(list(unique_types))\n",
        "\n",
        "  # combine the dummy columns with crime_df\n",
        "  new_df = pd.concat([df, unique_dummies], axis=1, )\n",
        "\n",
        "  # set all dummy values to 0\n",
        "  new_df.loc[:, unique_dummies.columns] = 0\n",
        "\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_unique_dummy_vals(df, prefix, max_types, unique_types):\n",
        "  new_df = df.copy(deep=True)\n",
        "\n",
        "  # for each entry\n",
        "  for i in df.index:\n",
        "      # for each bias column\n",
        "      for j in range(0, max_types):\n",
        "          dummy_type = new_df.loc[i][prefix + str(j)]\n",
        "          if dummy_type in unique_types:\n",
        "              # Use the iloc method to access the DataFrame by row and column indices\n",
        "              new_df.loc[i, dummy_type] = 1\n",
        "\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "3iif3kNHVcEi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multicol_parse(df, base_column, delimiter, replacement_dict):\n",
        "  new_df = df.copy(deep=True)\n",
        "\n",
        "  # get the max number of intermediate columns that must be created to hold the individual types that will be converted to dummy columns\n",
        "  max_cols = 5 #get_max_cols(df, column, delimiter)\n",
        "  print(max_cols)\n",
        "\n",
        "  # get a map of the intermediate type columns to be created\n",
        "  cols_map = get_type_cols_map(column, max_cols)\n",
        "  # create the intermediate type columns and set their values to np.nan\n",
        "  df[list(cols_map.values())] = np.nan\n",
        "  print(cols_map)\n",
        "\n",
        "  # split the base column into the intermediate columns\n",
        "  intermediate_df = split_multiCol(df, column, replacement_dict, cols_map, max_cols)\n",
        "  print(intermediate_df.iloc[1])\n",
        "\n",
        "  # create a list of the unique values from the intermediate columns\n",
        "  unique_values = get_unique_types(intermediate_df, list(cols_map.values()))\n",
        "  print(unique_values)\n",
        "\n",
        "  # go through the intermediate_df and create dummy columns for each unique value\n",
        "  # instantiate all columns' values to 0\n",
        "  new_df = get_unique_dummies(intermediate_df, unique_values)\n",
        "  # set the dummy columns' values for each entry\n",
        "  new_df = set_unique_dummy_vals(new_df, column, max_cols, unique_values)\n",
        "\n",
        "  return unique_values, cols_map.values(), new_df"
      ],
      "metadata": {
        "id": "ZBlzZXg662lW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA6an_zn7d6a"
      },
      "source": [
        "####Break up bias_desc column into dummy columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "nQusbBSc6VOV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias_replacement_dict = {\n",
        "    \"(Male)\": \"\",\n",
        "    \"(Female)\": \"\",\n",
        "    \"Lesbian, Gay, Bisexual, or Transgender (Mixed Group)\": \"LGBTQ\"\n",
        "}"
      ],
      "metadata": {
        "id": "jyJVyXdJ4aqu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7PYIcRq2hNT8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "4ef83cc7-6bf2-4f04-b934-95ed9d058f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "{0: '85 years and over0', 1: '85 years and over1', 2: '85 years and over2', 3: '85 years and over3', 4: '85 years and over4'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'85 years and over'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '85 years and over'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f74abb27184a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrime_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticol_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrime_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bias_desc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_replacement_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-c1acb28e4fcf>\u001b[0m in \u001b[0;36mmulticol_parse\u001b[0;34m(df, base_column, delimiter, replacement_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# split the base column into the intermediate columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mintermediate_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_multiCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-3c2dee0a43fb>\u001b[0m in \u001b[0;36msplit_multiCol\u001b[0;34m(df, column, replacementDict, multicol_cols, max_columns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_multiCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacementDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticol_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# and for each replacement to be made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplacementDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3868\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3870\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3871\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4253\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4255\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4256\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '85 years and over'"
          ]
        }
      ],
      "source": [
        "unique_biases, bias_cols, crime_df = multicol_parse(crime_df, \"bias_desc\", \";\", bias_replacement_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.head()"
      ],
      "metadata": {
        "id": "NXvQfE5X5Tkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Break victim_type up into dummy columns"
      ],
      "metadata": {
        "id": "2vPTabEpAOJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "-75DX2zHAn7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_victim_types, victim_cols, crime_df = multicol_parse(crime_df, \"victim_type\", \";\", {})"
      ],
      "metadata": {
        "id": "riHZGPmLATVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.head()"
      ],
      "metadata": {
        "id": "xPhCOyFFAbRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Break offense_name column up into dummy columns"
      ],
      "metadata": {
        "id": "gwbS-V0sRsf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "BdXn6PBFcr8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_offense_names, offense_cols, crime_df = multicol_parse(crime_df, \"offense_name\", \";\", {})"
      ],
      "metadata": {
        "id": "7TdBaaZMAwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.head()"
      ],
      "metadata": {
        "id": "hz0aQC1EbOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Break location_name column up into dummy columns"
      ],
      "metadata": {
        "id": "xzcbmQxTPtKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "lmy1JJlwP1ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_location_names, location_cols, crime_df = multicol_parse(crime_df, \"location_name\", \";\", {})"
      ],
      "metadata": {
        "id": "6HuAV-IGBErU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.head()"
      ],
      "metadata": {
        "id": "moMcNwjaBFxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s_ph3jfiVxi"
      },
      "source": [
        "###Create dummy columns for other categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wlb0NsBibhK"
      },
      "outputs": [],
      "source": [
        "other_categorical_cols = [\"agency_type_name\", \"division_name\", \"offender_race\", \"offender_ethnicity\",\n",
        "                            \"state_name\", \"multiple_offense\", \"multiple_bias\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS9mO9kYmujy"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(crime_df[other_categorical_cols]).astype(int)\n",
        "\n",
        "other_categorical_cols = dummies.columns\n",
        "\n",
        "# combine the dummy columns with crime_df\n",
        "crime_df = pd.concat([crime_df, dummies], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3UCnCYvnEca"
      },
      "outputs": [],
      "source": [
        "crime_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove intermediate columns"
      ],
      "metadata": {
        "id": "aLubPieQOEQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop half multiple bias, offense columns, since they're directly related to eachother\n",
        "crime_df.drop(columns=[\"multiple_offense_S\", \"multiple_bias_S\"])"
      ],
      "metadata": {
        "id": "lCc8d5K-PO3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 5 bias and 5 victim type columns from the non_na_cols\n",
        "cols_to_remove = bias_cols\\\n",
        "                  + victim_cols\\\n",
        "                  + offense_cols\\\n",
        "                  + location_cols\n",
        "\n",
        "crime_df.drop(columns=cols_to_remove)"
      ],
      "metadata": {
        "id": "UnRGY6e7ODY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_df.dtypes"
      ],
      "metadata": {
        "id": "sSGwynrvOaR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm5xNM5iePPX"
      },
      "source": [
        "##Combine the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_GEL4MtXNb-"
      },
      "outputs": [],
      "source": [
        "ethnicity_race_cols = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/county/ethnicity_race_col_names\")[\"One race\"]\n",
        "ethnicity_race_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUmBIXPtTTW0"
      },
      "outputs": [],
      "source": [
        "# Create new columns that combine data shared between all the city and county entries\n",
        "overlapping_columns = set(city_demo_df.columns).intersection(set(county_demo_df.columns))\n",
        "\n",
        "# include overlapping data and race and ethnicity data from the county dataset\n",
        "print(overlapping_columns.union(ethnicity_race_cols))\n",
        "demo_df = pd.concat([city_demo_df[list(overlapping_columns)], county_demo_df[list(overlapping_columns)]])\n",
        "# demo_df = pd.concat([demo_df, county_demo_df[list(ethnicity_race_cols)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Re5w41IIEq"
      },
      "outputs": [],
      "source": [
        "# combine the crime and population datasets\n",
        "merged_df = pd.merge(crime_df, demo_df, on=[\"pug_agency_name\", \"agency_type_name\", \"state_name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9s9cIrfq1o"
      },
      "source": [
        "##Clean the merged dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTWdt8V0VEhi"
      },
      "outputs": [],
      "source": [
        "# merged_df.drop([\"ori\", \"state_abbr\", \"population_group_code\", \"pub_agency_unit\",\n",
        "#                 \"nan\", \"Geography\", \"Percent!!Total population\", \"population_group_description\",\n",
        "#                 \"Geographic Area Name\", \"incident_date\", \"pub_agency_unit\"], axis=1, inplace=True)\n",
        "# merged_df.drop(list(merged_df.filter(regex = \"Margin of Error\")), axis = 1, inplace = True)\n",
        "\n",
        "merged_df.drop([\"ori\", \"state_abbr\", \"population_group_code\", \"pub_agency_unit\",\n",
        "                np.nan, \"Geography\", \"Percent!!Total population\", \"population_group_description\",\n",
        "                \"Geographic Area Name\", \"incident_date\", \"pub_agency_unit\", \"pug_agency_name\",\n",
        "                \"agency_type_name\", \"state_name\", \"division_name\", \"offender_race\", \"offender_ethnicity\",\n",
        "                \"location_name\", \"bias_desc\", \"victim_types\", \"multiple_offense\",\n",
        "                  \"multiple_bias\"], axis=1, inplace=True)\n",
        "\n",
        "# include offense_name in categorical data once you've refactored multicol parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYiZQtC32YWW"
      },
      "outputs": [],
      "source": [
        "# categorical variables\n",
        "binary_int_columns = list(unique_biases) + list(unique_victim_types) + list(unique_offense_names) + list(unique_location_names) + list(other_categorical_cols)\n",
        "int_columns = continuous_int_columns + binary_int_columns + [\"incident_month\", \"incident_day\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQTs7XZt5LD7"
      },
      "outputs": [],
      "source": [
        "# Replace all infinite values with np.nan\n",
        "merged_df = merged_df.replace([np.inf, -np.inf], np.nan).dropna(subset=int_columns).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6equL8vUAMzq"
      },
      "outputs": [],
      "source": [
        "# Convert the non_na_cols column to integers\n",
        "merged_df[int_columns] = merged_df[int_columns].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lON7Xj2bUrnC"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgSAMKYblqV3"
      },
      "source": [
        "##Individual Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9zD204ltAu"
      },
      "source": [
        "###Hate Crime Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwJOPMQEemwE"
      },
      "outputs": [],
      "source": [
        "crime_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Correlation matrix"
      ],
      "metadata": {
        "id": "ecZsjBHeaH-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_numerical_columns = crime_df[list(crime_df.select_dtypes(include=['int64', 'float64']).columns)].dropna()"
      ],
      "metadata": {
        "id": "bnh4AsvJZp0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix\n",
        "crime_corr = crime_df[crime_numerical_columns].dropna().corr()"
      ],
      "metadata": {
        "id": "X5kUrqFiae3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(160, 80))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(crime_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ixE6oLzladVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Other figures\n"
      ],
      "metadata": {
        "id": "-misImO6Z-5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsivn3pPdSRI"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('region_name').size().sort_values(ascending=False).plot.bar(color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.xticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7gqXZ5adQSX"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('agency_type_name').size().sort_values(ascending=False).plot.bar(color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.xticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAWBUw5cdDuf"
      },
      "outputs": [],
      "source": [
        "crime_df['total_offender_count'].plot.hist(bins=14, title='total_offender_count', logy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpKKqeYKdS9X"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('offender_race').size().sort_values(ascending=True).plot.barh(color=sns.palettes.mpl_palette('Dark2'), figsize=(10,10))\n",
        "plt.xticks(rotation=0, wrap=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p__cKs9lBeSj"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of unique biases and their counts\n",
        "unique_bias_counts = {}\n",
        "for bias in unique_biases:\n",
        "    unique_bias_counts[bias] = crime_df[bias].sum()\n",
        "\n",
        "# Sort the dictionary by values in descending order\n",
        "sorted_biases_counts = dict(sorted(unique_bias_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Create a bar chart of the sorted biases and their counts\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(x=list(sorted_biases_counts.keys()), y=list(sorted_biases_counts.values()), log=True)\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title(\"Frequency of Different Biases in Hate Crimes\", fontsize=16)\n",
        "plt.xlabel(\"Bias\", fontsize=14)\n",
        "plt.ylabel(\"Frequency (Log Scale)\", fontsize=14)\n",
        "\n",
        "# Rotate the x-axis labels for readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wJm4Vx5dBUZ"
      },
      "outputs": [],
      "source": [
        "crime_df['adult_victim_count'].plot.hist(bins=15, title='adult_victim_count', logy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdsK32w2dCMQ"
      },
      "outputs": [],
      "source": [
        "crime_df['juvenile_victim_count'].plot.hist(bins=10, title='juvenile_victim_count', logy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Regional distribution heatmaps"
      ],
      "metadata": {
        "id": "T5GiGjGvaVa2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bSbfApGYkZT"
      },
      "outputs": [],
      "source": [
        "ignore_list = [\"Not Specified\", \"Unknown\", \"Multiple\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0xr78kpdlJQ"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['offender_race'].value_counts()\n",
        "    for x_label, grp in crime_df.groupby('region_name')\n",
        "})\n",
        "\n",
        "# Drop less relevant columns for easier comparison with victim race heatmap\n",
        "for item in ignore_list:\n",
        "  df_2dhist = df_2dhist.drop(item)\n",
        "\n",
        "# Apply logarithmic transformation to the counts\n",
        "df_2dhist_log = df_2dhist.applymap(lambda x: 0 if x == 0 else np.log10(x))\n",
        "\n",
        "sns.heatmap(df_2dhist_log, cmap='viridis')\n",
        "plt.title(\"Logarithmic Frequency of Offender Races by Region\")\n",
        "plt.xlabel(\"Region Name\")\n",
        "plt.ylabel(\"Offender Race\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvQ1lreAFY9F"
      },
      "outputs": [],
      "source": [
        "# Get some victim races to start with\n",
        "unique_offender_races = crime_df['offender_race'].unique().tolist()\n",
        "unique_offender_races.remove(np.nan)\n",
        "\n",
        "# Drop categories absent from victim data\n",
        "for item in ignore_list:\n",
        "  unique_offender_races.remove(item)\n",
        "\n",
        "unique_victim_races = [\"Anti-\" + str(race) for race in unique_offender_races]\n",
        "\n",
        "print(unique_victim_races)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUBI_MwLKA8K"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Initialize an empty DataFrame with the correct index and columns\n",
        "df_2dhist = pd.DataFrame(index=unique_victim_races, columns=crime_df['region_name'].unique())\n",
        "\n",
        "# Iterate over each region and calculate the value counts for each victim race\n",
        "for region in df_2dhist.columns:\n",
        "    region_data = crime_df[crime_df['region_name'] == region]\n",
        "\n",
        "    for victim_race in unique_victim_races:\n",
        "        # Sum of occurrences of victim_race in the region\n",
        "        count = region_data[victim_race].sum()\n",
        "        # Update the DataFrame cell with the count\n",
        "        df_2dhist.loc[victim_race, region] = count\n",
        "\n",
        "# Convert DataFrame entries to numeric type and drop nan values\n",
        "df_2dhist = df_2dhist.apply(pd.to_numeric)\n",
        "df_2dhist.drop(columns=[np.nan], inplace=True)\n",
        "\n",
        "# Apply logarithmic transformation to the counts\n",
        "df_2dhist = df_2dhist.applymap(lambda x: 0 if x == 0 else np.log10(x))\n",
        "\n",
        "# Create the heatmap with logarithmic scale\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "\n",
        "# Give the plot a title and axis labels\n",
        "plt.title(\"Logarithmic Frequency of Victim Races by Region\")\n",
        "plt.xlabel(\"Region Name\")\n",
        "plt.ylabel(\"Victim Races\")\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjDExfePlzX2"
      },
      "source": [
        "###City Demographics Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Correlation matrix"
      ],
      "metadata": {
        "id": "RqbKV4ixayDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_numerical_cols = list(city_demo_df.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
      ],
      "metadata": {
        "id": "U0KLarRu4NKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix\n",
        "city_corr = city_demo_df[city_numerical_cols].corr()"
      ],
      "metadata": {
        "id": "It2TYbhQ4ZH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(80, 40))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(city_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T6gB1YoXIxGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Other figures"
      ],
      "metadata": {
        "id": "vfUqvO3Xa2um"
      }
    },
    {
      "source": [
        "city_demo_df.plot.scatter(x='20 to 24 years', y='75 to 84 years', logx=True, logy=True, s=40, alpha=.8, figsize=(16,8))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f6agpCK71P-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "city_demo_df['Total population'].plot.hist(bins=20, title='Total population', logy=True, figsize=(15,5))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ve1qjcRe1N7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_demo_df[age_groups].mean().plot.bar(logy=True, figsize=(16,8))\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Mean Population')\n",
        "plt.title('Mean Population by Age Group in City Demographics')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-ExLJbcroQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUTlcM64l91f"
      },
      "source": [
        "###County Demographics Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Correlation matrix"
      ],
      "metadata": {
        "id": "GsRxgRS_a6JV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MA9UiHVmB2m"
      },
      "outputs": [],
      "source": [
        "county_numerical_cols = list(county_demo_df.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix\n",
        "county_corr = county_demo_df[county_numerical_cols].corr()"
      ],
      "metadata": {
        "id": "W8fJKoip7jzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(100, 50))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(county_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9_Q5pIu5IrO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Other figures"
      ],
      "metadata": {
        "id": "w30H6uWia9pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df.plot.scatter(x='Under 5 years', y='85 years and over', logx=True, logy=True, s=40, alpha=.8, figsize=(16,8))"
      ],
      "metadata": {
        "id": "_ZOV2qpz796k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df[age_groups].mean().plot.bar(figsize=(20,5))\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Mean Population')\n",
        "plt.title('Mean Population by Age Group in City Demographics')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l18V6BHn8WZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vinl2fh1mEf2"
      },
      "source": [
        "##Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrV7bfizmKKG"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Correlation matrix"
      ],
      "metadata": {
        "id": "409NFkATbD_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op0QoPC9b2fP"
      },
      "outputs": [],
      "source": [
        "#convert the categorical columns to numerical ones and store the modified df as a new df for correlation analysis\n",
        "categorical_to_int_df = merged_df\n",
        "# Select only the numerical columns\n",
        "numerical_cols = categorical_to_int_df.select_dtypes(include=['int64', 'float64', np.number]).drop(labels=[\"incident_id\"], axis=1)\n",
        "# numerical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHN8Dby4dvXw"
      },
      "outputs": [],
      "source": [
        "# Create a correlation matrix\n",
        "merged_corr = merged_df[int_columns].dropna().corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(160, 80))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(merged_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0nuhpdPeyRp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Other figures"
      ],
      "metadata": {
        "id": "CwnBG9bwbG66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyei2nuHfjgI"
      },
      "outputs": [],
      "source": [
        "# merged_df[\"color\"] = merged_df[\"region_name\"].map({\n",
        "#     \"Midwest\": \"red\",\n",
        "#     \"West\": \"yellow\",\n",
        "#     \"Northeast\": \"green\",\n",
        "#     \"South\": \"blue\"\n",
        "# })\n",
        "# merged_df[\"size\"] = merged_df.groupby([\"incident_month\", \"region_name\"]).count().reset_index()[\"incident_id\"]\n",
        "\n",
        "# #remove the max row limit for altair\n",
        "# alt.data_transformers.disable_max_rows()\n",
        "\n",
        "# alt.Chart(merged_df.dropna()).mark_circle().encode(\n",
        "#     x=\"total_offender_count\",\n",
        "#     y=\"total_individual_victims\",\n",
        "#     color=alt.Color(\"color\", scale=None),\n",
        "#     size=\"size\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4_xXvj5uXc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "91_ky8rB7u13",
        "oYwpwcA8aqsI",
        "zouCbvk6_ccD",
        "Ns7wjEW0bLTL",
        "8hlE0Oe8bUbd",
        "yYKBzufDuKl5",
        "_jVRTkrMofxu",
        "8PgKZ6LO0rur",
        "HA6an_zn7d6a",
        "2vPTabEpAOJ8",
        "gwbS-V0sRsf8",
        "xzcbmQxTPtKj",
        "7s_ph3jfiVxi",
        "aLubPieQOEQt",
        "Fm5xNM5iePPX",
        "lON7Xj2bUrnC",
        "-misImO6Z-5i",
        "T5GiGjGvaVa2",
        "vfUqvO3Xa2um",
        "w30H6uWia9pW",
        "CwnBG9bwbG66"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}