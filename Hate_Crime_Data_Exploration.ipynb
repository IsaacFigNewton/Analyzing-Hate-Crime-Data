{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/Analyzing-Hate-Crime-Data/blob/main/Hate_Crime_Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOiwCxBSfrVr"
      },
      "source": [
        "#TODO:\n",
        "####Refactor multi-column parsing code for bias_desc, victim_types, and offense_name\n",
        "####include offense_name in categorical data once you've refactored multicol parsing\n",
        "####Add the ethnicity_race_cols to demo_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1drJWUkF4odQ"
      },
      "source": [
        "#Import Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXLq0TJ6dd75"
      },
      "source": [
        "###Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YVKUvdikdVxP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH0vaj3TdsQ_"
      },
      "source": [
        "###Import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bCX3rTyyddMq"
      },
      "outputs": [],
      "source": [
        "crime_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/hate_crime/hate_crime.csv\", on_bad_lines='skip')\n",
        "city_demo_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/city/ACSST1Y2022.S0101-Data.csv\", on_bad_lines='skip')\n",
        "county_demo_df = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/county/ACSDP1Y2022.DP05-Data.csv\", on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMmPAMsD4kTu"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n5X6ugA1wk7E"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ad1nsDl75WNg"
      },
      "outputs": [],
      "source": [
        "#only consider 2022 crime data from cities and counties\n",
        "crime_df = crime_df[(crime_df['data_year'] == 2022) & ((crime_df['agency_type_name'] == \"City\") | (crime_df['agency_type_name'] == \"County\"))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_ky8rB7u13"
      },
      "source": [
        "##Clean city and county demographic datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MvzdSWFyzeW_"
      },
      "outputs": [],
      "source": [
        "# use the entries of the first row as the column headers for easier management\n",
        "def fixHeaders(df):\n",
        "  new_headers = df.iloc[0]\n",
        "  new_df = df[1:]\n",
        "  new_df.columns = new_headers\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9WA3B15xen01"
      },
      "outputs": [],
      "source": [
        "#fix the headers\n",
        "city_demo_df = fixHeaders(city_demo_df)\n",
        "county_demo_df = fixHeaders(county_demo_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create area columns"
      ],
      "metadata": {
        "id": "oYwpwcA8aqsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wptIjBBS43De"
      },
      "outputs": [],
      "source": [
        "def split_area_name(area):\n",
        "    result = [np.nan, np.nan, np.nan]\n",
        "    if \", \" in area:\n",
        "        result = area.split(\", \") + [np.nan]\n",
        "    if \" city\" in result[0].lower():\n",
        "        result[0] = result[0][0:-5]\n",
        "        result[2] = result[1]\n",
        "        result[1] = \"City\"\n",
        "    elif \" county\" in result[0].lower():\n",
        "        result[0] = result[0][0:-7]\n",
        "        result[2] = result[1]\n",
        "        result[1] = \"County\"\n",
        "\n",
        "    return result + [np.nan] * (3 - len(result))\n",
        "\n",
        "def splitArea(df):\n",
        "    df[[\"pug_agency_name\", \"agency_type_name\", \"state_name\"]] = df[\"Geographic Area Name\"].map(split_area_name).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vDxDbqFs5ax-"
      },
      "outputs": [],
      "source": [
        "splitArea(city_demo_df)\n",
        "splitArea(county_demo_df)\n",
        "\n",
        "# city_demo_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zouCbvk6_ccD"
      },
      "source": [
        "###Fix column names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZfZR5Bv1OalM"
      },
      "outputs": [],
      "source": [
        "def fix_column_names(df):\n",
        "  removalList = {\"SEX AND AGE!!\", \"SEX AND \", \"Estimate!!\", \"Total!!\", \"Total population!!\", \"CITIZEN, VOTING AGE POPULATION!!\", \"AGE!!\"}\n",
        "\n",
        "  new_cols = []\n",
        "  for col in df.columns:\n",
        "    col = str(col)\n",
        "    for term in removalList:\n",
        "      if term in col:\n",
        "        col = col.replace(term, \"\")\n",
        "    new_cols.append(col)\n",
        "\n",
        "  return new_cols\n",
        "\n",
        "county_demo_df.columns = fix_column_names(county_demo_df)\n",
        "city_demo_df.columns = fix_column_names(city_demo_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Add and remove columns to line the dataframes up"
      ],
      "metadata": {
        "id": "Ns7wjEW0bLTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oLuvQpJzkLlT"
      },
      "outputs": [],
      "source": [
        "#combine the city_df age-related columns as needed to merge better with the county_demo_df\n",
        "def combine_city_df_cols(df_s0101):\n",
        "    formatted_df = df_s0101.copy(deep=True)\n",
        "\n",
        "    formatted_df['25 to 34 years'] = df_s0101['25 to 29 years'] + df_s0101['30 to 34 years']\n",
        "    formatted_df['35 to 44 years'] = df_s0101['35 to 39 years'] + df_s0101['40 to 44 years']\n",
        "    formatted_df['45 to 54 years'] = df_s0101['45 to 49 years'] + df_s0101['50 to 54 years']\n",
        "    formatted_df['65 to 74 years'] = df_s0101['65 to 69 years'] + df_s0101['70 to 74 years']\n",
        "    formatted_df['75 to 84 years'] = df_s0101['75 to 79 years'] + df_s0101['80 to 84 years']\n",
        "\n",
        "    return formatted_df\n",
        "\n",
        "city_demo_df = combine_city_df_cols(city_demo_df)\n",
        "# city_demo_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yUk3NyAxVAc2"
      },
      "outputs": [],
      "source": [
        "# fix any missing values\n",
        "city_demo_df = city_demo_df.replace(\"(X)\", np.nan)\n",
        "county_demo_df = county_demo_df.replace(\"(X)\", np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop all margin of error columns from the city and county dataframes\n",
        "def drop_cols_containing(df, pattern):\n",
        "    df.drop(columns=list(df.filter(regex = pattern)), inplace = True)\n",
        "\n",
        "drop_cols_containing(city_demo_df, \"Margin of Error|SUMMARY INDICATORS|PERCENT ALLOCATED|SELECTED AGE CATEGORIES\")\n",
        "drop_cols_containing(county_demo_df, \"Margin of Error|SUMMARY INDICATORS|PERCENT ALLOCATED\")\n",
        "\n",
        "# city_demo_df.dtypes"
      ],
      "metadata": {
        "id": "G7cTb7FkY-Rl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fix column data types"
      ],
      "metadata": {
        "id": "8hlE0Oe8bUbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = county_demo_df.columns.value_counts()\n",
        "mask = counts > 1\n",
        "duplicates = list(counts[mask].index)\n",
        "county_demo_df[duplicates].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALM0fmMBEJAC",
        "outputId": "c9226474-44a2-4a8f-fd03-510d6f91faa2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65 years and over                   object\n",
              "65 years and over                   object\n",
              "18 years and over                   object\n",
              "18 years and over                   object\n",
              "Percent!!65 years and over          object\n",
              "Percent!!65 years and over          object\n",
              "Percent!!RACE!!One race             object\n",
              "Percent!!RACE!!One race             object\n",
              "Percent!!RACE!!Two or More Races    object\n",
              "Percent!!RACE!!Two or More Races    object\n",
              "Percent!!18 years and over          object\n",
              "Percent!!18 years and over          object\n",
              "RACE!!One race                      object\n",
              "RACE!!One race                      object\n",
              "RACE!!Two or More Races             object\n",
              "RACE!!Two or More Races             object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df = county_demo_df.loc[:, ~county_demo_df.columns.duplicated(keep='last')]"
      ],
      "metadata": {
        "id": "fAyrAm9yFOvt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_percent_age_cols = list(set(list(city_demo_df.filter(regex = \"Percent\")) + list(city_demo_df.filter(regex = \"ratio\")) + list(city_demo_df.filter(regex = \"years\"))))\n",
        "county_percent_age_cols = list(set(list(county_demo_df.filter(regex = \"Percent\")) + list(county_demo_df.filter(regex = \"ratio\")) + list(county_demo_df.filter(regex = \"years\"))))\n",
        "\n",
        "for column in city_percent_age_cols:\n",
        "    city_demo_df = city_demo_df[~city_demo_df[column].astype(str).str.contains('N')]\n",
        "\n",
        "for column in county_percent_age_cols:\n",
        "    county_demo_df = county_demo_df[~county_demo_df[column].astype(str).str.contains('N')]"
      ],
      "metadata": {
        "id": "jZYIR6d-iPJR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_demo_df[city_percent_age_cols] = city_demo_df[city_percent_age_cols].astype(float)\n",
        "county_demo_df[county_percent_age_cols] = county_demo_df[county_percent_age_cols].astype(float)"
      ],
      "metadata": {
        "id": "ZrdOlh7lGDt9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the quantitative columns to int types\n",
        "# continuous quantitative variables\n",
        "continuous_int_columns = [\"Total population\", \"Under 5 years\", \"5 to 9 years\", \"45 to 54 years\", \"10 to 14 years\", \"75 to 84 years\", \"60 to 64 years\",\\\n",
        "                      \"25 to 34 years\", \"15 to 19 years\", \"20 to 24 years\", \"35 to 44 years\", \"55 to 59 years\", \"65 to 74 years\",\\\n",
        "                      \"85 years and over\"]# + list(city_demo_df.filter(regex = \"Total population\"))))\n",
        "\n",
        "for column in continuous_int_columns:\n",
        "    city_demo_df = city_demo_df[~city_demo_df[column].astype(str).str.contains('N')]\n",
        "    county_demo_df = county_demo_df[~county_demo_df[column].astype(str).str.contains('N')]\n",
        "\n",
        "city_demo_df[continuous_int_columns] = city_demo_df[continuous_int_columns].astype(int)\n",
        "county_demo_df[continuous_int_columns] = county_demo_df[continuous_int_columns].astype(int)"
      ],
      "metadata": {
        "id": "Q8BIgfCuZ3cS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_groups = ['Under 5 years', '5 to 9 years', '10 to 14 years', '15 to 19 years', '20 to 24 years', '25 to 34 years', '35 to 44 years', '45 to 54 years', '55 to 59 years', '60 to 64 years', '65 to 74 years', '75 to 84 years', '85 years and over']"
      ],
      "metadata": {
        "id": "fdq9xxRHsFiM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Drop more invalid data"
      ],
      "metadata": {
        "id": "yYKBzufDuKl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# city_demo_df.head()"
      ],
      "metadata": {
        "id": "q4ug9w0EwJ4z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for group in age_groups:\n",
        "#     city_demo_df = city_demo_df[city_demo_df[group] < 10**7]\n",
        "#     county_demo_df = county_demo_df[county_demo_df[group] < 10**7]\n",
        "\n",
        "# city_demo_df.head()"
      ],
      "metadata": {
        "id": "fmQBOYl5tgxM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxkvqx0R8A29"
      },
      "source": [
        "##Clean crime dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgKZ6LO0rur"
      },
      "source": [
        "###Important functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "whQdcLBmrJus"
      },
      "outputs": [],
      "source": [
        "def get_max_cols(df, column, character, prefix):\n",
        "    max = 0\n",
        "\n",
        "    # for each entry\n",
        "    for i in df.index:\n",
        "      val = df.loc[i, column]\n",
        "\n",
        "      if val != np.nan and type(val) != float:\n",
        "        # Count occurrences of character in the specified column\n",
        "        # then store the result in the prefix + \"_req_cols\" column\n",
        "        # try:\n",
        "        temp = len(str(val).split(character))\n",
        "        # except:\n",
        "        #     print(val)\n",
        "\n",
        "        if temp > max:\n",
        "          max = temp\n",
        "          # print(max)\n",
        "\n",
        "      else:\n",
        "        print(\"val was np.nan or float; unable to parse\")\n",
        "\n",
        "      return max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d-HY4ZyLA4nt"
      },
      "outputs": [],
      "source": [
        "max_cols = 0\n",
        "\n",
        "def split_types(val):\n",
        "  # break up the col_vals into individual col_vals\n",
        "  col_vals = str(val).split(\";\")\n",
        "  for i in range(len(col_vals)):\n",
        "      col_vals[i] = col_vals[i].strip()\n",
        "\n",
        "  # add placeholders\n",
        "  col_vals = col_vals + [np.nan] * (max_cols - len(col_vals))\n",
        "\n",
        "  return col_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Y5Xs8UBRqRtP"
      },
      "outputs": [],
      "source": [
        "def split_multiCol(df, column, replacementDict, max_columns):\n",
        "    for i in df.index:\n",
        "      if type(df.loc[i, column]) == str:\n",
        "        # and for each replacement to be made\n",
        "        for before, after in replacementDict.items():\n",
        "          df.loc[i, column] = df.loc[i, column].replace(before, after)\n",
        "        # remove extra whitespace\n",
        "        df.loc[i, column].strip()\n",
        "\n",
        "    # change max_cols here, since you can't do it from the mapping function\n",
        "    max_cols = max_columns\n",
        "\n",
        "    return df[column].map(split_types).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UtuXXR3g0nwq"
      },
      "outputs": [],
      "source": [
        "def get_unique_types(df, type_cols):\n",
        "  all_types = []\n",
        "  for col in type_cols:\n",
        "    all_types.extend(df[col].unique())\n",
        "\n",
        "  # create a set of just the unique ones\n",
        "  unique_types = set(all_types)\n",
        "\n",
        "  # remove 0, which got in there somehow\n",
        "  unique_types.remove(0)\n",
        "\n",
        "  return unique_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SdJyFKJfzdHr"
      },
      "outputs": [],
      "source": [
        "def get_unique_dummies(df, unique_types):\n",
        "  unique_dummies = pd.get_dummies(list(unique_types))\n",
        "\n",
        "  # combine the dummy columns with crime_df\n",
        "  df = pd.concat([df, unique_dummies], axis=1)\n",
        "\n",
        "  # set all dummy values to 0\n",
        "  df.loc[:, unique_dummies.columns] = 0\n",
        "\n",
        "  return unique_dummies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jVRTkrMofxu"
      },
      "source": [
        "###Break up incident date information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LWs_l73xYNHa"
      },
      "outputs": [],
      "source": [
        "def split_incident_date(date):\n",
        "    result = date.split(\"-\")\n",
        "\n",
        "    return result + [np.nan] * (3 - len(result))\n",
        "\n",
        "crime_df[[\"data_year\", \"incident_month\", \"incident_day\"]] = crime_df[\"incident_date\"].map(split_incident_date).apply(pd.Series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA6an_zn7d6a"
      },
      "source": [
        "###Break up bias descriptions into dummy columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiZ4Oczk1F9_"
      },
      "source": [
        "####Parse the bias_desc's into their own individual columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TvKgWiX_QI1v"
      },
      "outputs": [],
      "source": [
        "# TODO: find max bias count in mapping function below?\n",
        "max_bias_count = 5\n",
        "\n",
        "def split_bias_desc(desc):\n",
        "  # break up the biases into individual biases\n",
        "  biases = str(desc).split(\";\")\n",
        "  for i in range(len(biases)):\n",
        "      biases[i] = biases[i]\\\n",
        "                    .replace(\"(Male)\", \"\")\\\n",
        "                    .replace(\"(Female)\", \"\")\\\n",
        "                    .replace(\"Lesbian, Gay, Bisexual, or Transgender (Mixed Group)\", \"LGBTQ\")\\\n",
        "                    .strip()\n",
        "\n",
        "  # add placeholders\n",
        "  biases = biases + [np.nan] * (max_bias_count - len(biases))\n",
        "\n",
        "  return biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "J3UqsEOSP2Zk"
      },
      "outputs": [],
      "source": [
        "bias_cols = [\"bias\" + str(i) for i in range(max_bias_count)]\n",
        "\n",
        "crime_df[bias_cols] = crime_df[\"bias_desc\"]\\\n",
        "                          .map(split_bias_desc)\\\n",
        "                          .apply(pd.Series)\n",
        "\n",
        "# remove the \"0\" column, which I'm not entirely sure why exists\n",
        "if \"0\" in crime_df.columns:\n",
        "  crime_df = crime_df.drop(columns=[\"0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yc8VmLG_vlP4"
      },
      "outputs": [],
      "source": [
        "# max_bias_desc_cols = get_max_cols(crime_df, \"bias_desc\", \";\", \"bias_desc\")\n",
        "\n",
        "# bias_cols = [\"bias\" + str(i) for i in range(max_bias_desc_cols)]\n",
        "\n",
        "# def split_bias_desc(val):\n",
        "#   return split_multiCol(val, {\"(Male)\":\"\", \"(Female)\":\"\", \"Lesbian, Gay, Bisexual, or Transgender (Mixed Group)\": \"LGBTQ\"}, max_bias_desc_cols)\n",
        "\n",
        "# crime_df[bias_cols] = crime_df[\"bias_desc\"]\\\n",
        "#                           .map(split_bias_desc)\\\n",
        "#                           .apply(pd.Series)\n",
        "\n",
        "# # remove the \"0\" column, which I'm not entirely sure why exists\n",
        "# if \"0\" in crime_df.columns:\n",
        "#   crime_df = crime_df.drop(columns=[\"0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hhocTJ_N5q"
      },
      "source": [
        "####Get a list of all unique biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kTkdUjjzZgB3"
      },
      "outputs": [],
      "source": [
        "all_biases = []\n",
        "for col in bias_cols:\n",
        "  all_biases.extend(crime_df[col].unique())\n",
        "\n",
        "# create a set of just the unique ones\n",
        "unique_biases = set(all_biases)\n",
        "\n",
        "# remove 0, which got in there somehow\n",
        "unique_biases.remove(np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtnNQnfR0_uu",
        "outputId": "e9d14291-3b24-4f06-fec9-3504c2408c74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Anti-American Indian or Alaska Native',\n",
              " 'Anti-Arab',\n",
              " 'Anti-Asian',\n",
              " 'Anti-Atheism/Agnosticism',\n",
              " 'Anti-Bisexual',\n",
              " 'Anti-Black or African American',\n",
              " 'Anti-Buddhist',\n",
              " 'Anti-Catholic',\n",
              " 'Anti-Church of Jesus Christ',\n",
              " 'Anti-Eastern Orthodox (Russian, Greek, Other)',\n",
              " 'Anti-Female',\n",
              " 'Anti-Gay',\n",
              " 'Anti-Gender Non-Conforming',\n",
              " 'Anti-Heterosexual',\n",
              " 'Anti-Hindu',\n",
              " 'Anti-Hispanic or Latino',\n",
              " 'Anti-Islamic (Muslim)',\n",
              " \"Anti-Jehovah's Witness\",\n",
              " 'Anti-Jewish',\n",
              " 'Anti-LGBTQ',\n",
              " 'Anti-Lesbian',\n",
              " 'Anti-Male',\n",
              " 'Anti-Mental Disability',\n",
              " 'Anti-Multiple Races, Group',\n",
              " 'Anti-Multiple Religions, Group',\n",
              " 'Anti-Native Hawaiian or Other Pacific Islander',\n",
              " 'Anti-Other Christian',\n",
              " 'Anti-Other Race/Ethnicity/Ancestry',\n",
              " 'Anti-Other Religion',\n",
              " 'Anti-Physical Disability',\n",
              " 'Anti-Protestant',\n",
              " 'Anti-Sikh',\n",
              " 'Anti-Transgender',\n",
              " 'Anti-White'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "unique_biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXLlSu1U_T9I"
      },
      "source": [
        "####Create dummy columns for all hate crime biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mn4wwlX1jsOk"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(list(unique_biases))\n",
        "\n",
        "# combine the dummy columns with crime_df\n",
        "crime_df = pd.concat([crime_df, dummies], axis=1)\n",
        "\n",
        "# set all dummy values to 0\n",
        "crime_df.loc[:, dummies.columns] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7PYIcRq2hNT8"
      },
      "outputs": [],
      "source": [
        "#set dummy column values according to biases parsed from bias_desc\n",
        "\n",
        "# for each entry\n",
        "for i in crime_df.index:\n",
        "    # for each bias column\n",
        "    for j in range(0, 5):\n",
        "        bias = crime_df.loc[i][\"bias\" + str(j)]\n",
        "        if bias in unique_biases:\n",
        "            # Use the iloc method to access the DataFrame by row and column indices\n",
        "            crime_df.loc[i, bias] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4P7-iemnZMl"
      },
      "source": [
        "###Break victim_types column up into dummy columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-TZQTyxE-RDn"
      },
      "outputs": [],
      "source": [
        "crime_df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ASyJ510639"
      },
      "source": [
        "####Parse the victim_types into their own individual columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qNSCaCkJE3QF"
      },
      "outputs": [],
      "source": [
        "# TODO: find max bias count in mapping function below?\n",
        "max_bias_count = 5\n",
        "\n",
        "def split_victim_types(desc):\n",
        "  # break up the victims into individual victims\n",
        "  victims = str(desc).split(\";\")\n",
        "  for i in range(len(victims)):\n",
        "      victims[i] = victims[i].strip()\n",
        "\n",
        "  # add placeholders\n",
        "  victims = victims + [np.nan] * (max_bias_count - len(victims))\n",
        "\n",
        "  return victims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09MyremgE4b-"
      },
      "outputs": [],
      "source": [
        "victim_type_cols = [\"victim_type_\" + str(i) for i in range(max_bias_count)]\n",
        "\n",
        "crime_df[victim_type_cols] = crime_df[\"victim_types\"]\\\n",
        "                          .map(split_victim_types)\\\n",
        "                          .apply(pd.Series)\n",
        "\n",
        "# remove the \"0\" column, which I'm not entirely sure why exists\n",
        "if np.nan in crime_df.columns:\n",
        "  crime_df = crime_df.drop(columns=[np.nan])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKqzDrg0pBTO"
      },
      "outputs": [],
      "source": [
        "# max_victim_type_cols = get_max_cols(crime_df, \"victim_types\", \";\", \"types\")\n",
        "# print(max_victim_type_cols)\n",
        "# print(\"NOTE: GET_MAX_COLS IS BROKEN, PLEASE FIX\")\n",
        "\n",
        "# victim_type_cols = [\"victim_type\" + str(i) for i in range(max_victim_type_cols)]\n",
        "\n",
        "# # print(set(victim_type_cols).difference(set))\n",
        "# crime_df[victim_type_cols] = 0\n",
        "# victim_df = split_multiCol(crime_df, \"victim_types\", {}, max_victim_type_cols)\n",
        "# print(victim_df.columns)\n",
        "# # print(crime_df.columns)\n",
        "# # crime_df[victim_type_cols] = victim_df\n",
        "\n",
        "# # remove the \"0\" column, which I'm not entirely sure why exists\n",
        "# if \"0\" in crime_df.columns:\n",
        "#   crime_df = crime_df.drop(columns=[\"0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA_zceh1oTPM"
      },
      "source": [
        "####Get a list of all unique victim types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svH9cD7F0Xeq"
      },
      "outputs": [],
      "source": [
        "# unique_victim_types = get_unique_types(crime_df, victim_type_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykh_LNceoWES"
      },
      "outputs": [],
      "source": [
        "all_victim_types = []\n",
        "for col in victim_type_cols:\n",
        "  all_victim_types.extend(crime_df[col].unique())\n",
        "\n",
        "# create a set of just the unique ones\n",
        "unique_victims = set(all_victim_types)\n",
        "\n",
        "# remove np.nan if it's in there\n",
        "if np.nan in unique_victims:\n",
        "  unique_victims.remove(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzeM5TSsoKeU"
      },
      "source": [
        "####Create dummy columns for all victim types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xg9rWdfzTVA"
      },
      "outputs": [],
      "source": [
        "# dummies = get_unique_dummies(crime_df, unique_victims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlaF8eekn2Nb"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(list(unique_victims))\n",
        "\n",
        "# combine the dummy columns with crime_df\n",
        "crime_df = pd.concat([crime_df, dummies], axis=1)\n",
        "\n",
        "# set all dummy values to 0\n",
        "crime_df.loc[:, dummies.columns] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsF_vsoXpVmY"
      },
      "outputs": [],
      "source": [
        "#set dummy column values according to biases parsed from bias_desc\n",
        "\n",
        "# for each entry\n",
        "for i in crime_df.index:\n",
        "    # for each bias column\n",
        "    for j in range(0, max_bias_count):\n",
        "        victim = crime_df.loc[i][\"victim_type_\" + str(j)]\n",
        "        if victim in unique_victims:\n",
        "            # Use the iloc method to access the DataFrame by row and column indices\n",
        "            crime_df.loc[i, victim] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL_LlmFVpXRA"
      },
      "outputs": [],
      "source": [
        "crime_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s_ph3jfiVxi"
      },
      "source": [
        "###Create dummy columns for other categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wlb0NsBibhK"
      },
      "outputs": [],
      "source": [
        "nonbias_categorical_cols = [\"agency_type_name\", \"division_name\", \"offender_race\", \"offender_ethnicity\",\n",
        "                            \"offense_name\", \"location_name\", \"multiple_offense\", \"multiple_bias\"] # + [\"state_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS9mO9kYmujy"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(crime_df[nonbias_categorical_cols])\n",
        "\n",
        "nonbias_categorical_cols = dummies.columns\n",
        "\n",
        "# combine the dummy columns with crime_df\n",
        "crime_df = pd.concat([crime_df, dummies], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3UCnCYvnEca"
      },
      "outputs": [],
      "source": [
        "crime_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm5xNM5iePPX"
      },
      "source": [
        "##Combine the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_GEL4MtXNb-"
      },
      "outputs": [],
      "source": [
        "ethnicity_race_cols = pd.read_csv(\"https://raw.githubusercontent.com/IsaacFigNewton/Analyzing-Hate-Crime-Data/main/demographics/county/ethnicity_race_col_names\")[\"One race\"]\n",
        "ethnicity_race_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUmBIXPtTTW0"
      },
      "outputs": [],
      "source": [
        "# Create new columns that combine data shared between all the city and county entries\n",
        "overlapping_columns = set(city_demo_df.columns).intersection(set(county_demo_df.columns))\n",
        "\n",
        "# include overlapping data and race and ethnicity data from the county dataset\n",
        "print(overlapping_columns.union(ethnicity_race_cols))\n",
        "demo_df = pd.concat([city_demo_df[list(overlapping_columns)], county_demo_df[list(overlapping_columns)]])\n",
        "# demo_df = pd.concat([demo_df, county_demo_df[list(ethnicity_race_cols)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Re5w41IIEq"
      },
      "outputs": [],
      "source": [
        "# combine the crime and population datasets\n",
        "merged_df = pd.merge(crime_df, demo_df, on=[\"pug_agency_name\", \"agency_type_name\", \"state_name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9s9cIrfq1o"
      },
      "source": [
        "##Clean the merged dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTWdt8V0VEhi"
      },
      "outputs": [],
      "source": [
        "# merged_df.drop([\"ori\", \"state_abbr\", \"population_group_code\", \"pub_agency_unit\",\n",
        "#                 \"nan\", \"Geography\", \"Percent!!Total population\", \"population_group_description\",\n",
        "#                 \"Geographic Area Name\", \"incident_date\", \"pub_agency_unit\"], axis=1, inplace=True)\n",
        "# merged_df.drop(list(merged_df.filter(regex = \"Margin of Error\")), axis = 1, inplace = True)\n",
        "\n",
        "merged_df.drop([\"ori\", \"state_abbr\", \"population_group_code\", \"pub_agency_unit\",\n",
        "                np.nan, \"Geography\", \"Percent!!Total population\", \"population_group_description\",\n",
        "                \"Geographic Area Name\", \"incident_date\", \"pub_agency_unit\", \"pug_agency_name\",\n",
        "                \"agency_type_name\", \"state_name\", \"division_name\", \"offender_race\", \"offender_ethnicity\",\n",
        "                \"location_name\", \"bias_desc\", \"victim_types\", \"multiple_offense\",\n",
        "                  \"multiple_bias\"], axis=1, inplace=True)\n",
        "\n",
        "# include offense_name in categorical data once you've refactored multicol parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYiZQtC32YWW"
      },
      "outputs": [],
      "source": [
        "# categorical variables\n",
        "binary_int_columns = list(unique_biases) + list(nonbias_categorical_cols)\n",
        "int_columns = continuous_int_columns + binary_int_columns + [\"incident_month\", \"incident_day\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQTs7XZt5LD7"
      },
      "outputs": [],
      "source": [
        "# Replace all infinite values with np.nan\n",
        "non_na_cols = merged_df.replace([np.inf, -np.inf], np.nan).dropna(subset=int_columns).columns\n",
        "\n",
        "# Drop the 5 bias columns from the non_na_cols\n",
        "cols_to_remove = [\"bias\" + str(i) for i in range(max_bias_count)] + [\"victim_type_\" + str(i) for i in range(max_bias_count)]\n",
        "\n",
        "non_na_cols.drop(cols_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6equL8vUAMzq"
      },
      "outputs": [],
      "source": [
        "# Convert the non_na_cols column to integers\n",
        "merged_df[int_columns] = merged_df[int_columns].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lON7Xj2bUrnC"
      },
      "source": [
        "# Explore the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgSAMKYblqV3"
      },
      "source": [
        "##Individual Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9zD204ltAu"
      },
      "source": [
        "###Hate Crime Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwJOPMQEemwE"
      },
      "outputs": [],
      "source": [
        "crime_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsivn3pPdSRI"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('region_name').size().sort_values(ascending=False).plot.bar(color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.xticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7gqXZ5adQSX"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('agency_type_name').size().sort_values(ascending=False).plot.bar(color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.xticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAWBUw5cdDuf"
      },
      "outputs": [],
      "source": [
        "crime_df['total_offender_count'].plot.hist(bins=14, title='total_offender_count', logy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpKKqeYKdS9X"
      },
      "outputs": [],
      "source": [
        "crime_df.groupby('offender_race').size().sort_values(ascending=True).plot.barh(color=sns.palettes.mpl_palette('Dark2'), figsize=(10,10))\n",
        "plt.xticks(rotation=0, wrap=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p__cKs9lBeSj"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of unique biases and their counts\n",
        "unique_bias_counts = {}\n",
        "for bias in unique_biases:\n",
        "    unique_bias_counts[bias] = crime_df[bias].sum()\n",
        "\n",
        "# Sort the dictionary by values in descending order\n",
        "sorted_biases_counts = dict(sorted(unique_bias_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Create a bar chart of the sorted biases and their counts\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(x=list(sorted_biases_counts.keys()), y=list(sorted_biases_counts.values()), log=True)\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title(\"Frequency of Different Biases in Hate Crimes\", fontsize=16)\n",
        "plt.xlabel(\"Bias\", fontsize=14)\n",
        "plt.ylabel(\"Frequency (Log Scale)\", fontsize=14)\n",
        "\n",
        "# Rotate the x-axis labels for readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wJm4Vx5dBUZ"
      },
      "outputs": [],
      "source": [
        "crime_df['adult_victim_count'].plot.hist(bins=15, title='adult_victim_count', logy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdsK32w2dCMQ"
      },
      "outputs": [],
      "source": [
        "crime_df['juvenile_victim_count'].plot.hist(bins=10, title='juvenile_victim_count', logy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bSbfApGYkZT"
      },
      "outputs": [],
      "source": [
        "ignore_list = [\"Not Specified\", \"Unknown\", \"Multiple\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0xr78kpdlJQ"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['offender_race'].value_counts()\n",
        "    for x_label, grp in crime_df.groupby('region_name')\n",
        "})\n",
        "\n",
        "# Drop less relevant columns for easier comparison with victim race heatmap\n",
        "for item in ignore_list:\n",
        "  df_2dhist = df_2dhist.drop(item)\n",
        "\n",
        "# Apply logarithmic transformation to the counts\n",
        "df_2dhist_log = df_2dhist.applymap(lambda x: 0 if x == 0 else np.log10(x))\n",
        "\n",
        "sns.heatmap(df_2dhist_log, cmap='viridis')\n",
        "plt.title(\"Logarithmic Frequency of Offender Races by Region\")\n",
        "plt.xlabel(\"Region Name\")\n",
        "plt.ylabel(\"Offender Race\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvQ1lreAFY9F"
      },
      "outputs": [],
      "source": [
        "# Get some victim races to start with\n",
        "unique_offender_races = crime_df['offender_race'].unique().tolist()\n",
        "unique_offender_races.remove(np.nan)\n",
        "\n",
        "# Drop categories absent from victim data\n",
        "for item in ignore_list:\n",
        "  unique_offender_races.remove(item)\n",
        "\n",
        "unique_victim_races = [\"Anti-\" + str(race) for race in unique_offender_races]\n",
        "\n",
        "print(unique_victim_races)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUBI_MwLKA8K"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Initialize an empty DataFrame with the correct index and columns\n",
        "df_2dhist = pd.DataFrame(index=unique_victim_races, columns=crime_df['region_name'].unique())\n",
        "\n",
        "# Iterate over each region and calculate the value counts for each victim race\n",
        "for region in df_2dhist.columns:\n",
        "    region_data = crime_df[crime_df['region_name'] == region]\n",
        "\n",
        "    for victim_race in unique_victim_races:\n",
        "        # Sum of occurrences of victim_race in the region\n",
        "        count = region_data[victim_race].sum()\n",
        "        # Update the DataFrame cell with the count\n",
        "        df_2dhist.loc[victim_race, region] = count\n",
        "\n",
        "# Convert DataFrame entries to numeric type and drop nan values\n",
        "df_2dhist = df_2dhist.apply(pd.to_numeric)\n",
        "df_2dhist.drop(columns=[np.nan], inplace=True)\n",
        "\n",
        "# Apply logarithmic transformation to the counts\n",
        "df_2dhist = df_2dhist.applymap(lambda x: 0 if x == 0 else np.log10(x))\n",
        "\n",
        "# Create the heatmap with logarithmic scale\n",
        "sns.heatmap(df_2dhist, cmap='viridis')\n",
        "\n",
        "# Give the plot a title and axis labels\n",
        "plt.title(\"Logarithmic Frequency of Victim Races by Region\")\n",
        "plt.xlabel(\"Region Name\")\n",
        "plt.ylabel(\"Victim Races\")\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjDExfePlzX2"
      },
      "source": [
        "###City Demographics Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_numerical_cols = list(city_demo_df.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
      ],
      "metadata": {
        "id": "U0KLarRu4NKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix\n",
        "city_corr = city_demo_df[city_numerical_cols].corr()"
      ],
      "metadata": {
        "id": "It2TYbhQ4ZH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(160, 80))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(city_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T6gB1YoXIxGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "city_demo_df.plot.scatter(x='20 to 24 years', y='75 to 84 years', logx=True, logy=True, s=40, alpha=.8, figsize=(16,8))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f6agpCK71P-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "city_demo_df['Total population'].plot.hist(bins=20, title='Total population', logy=True, figsize=(15,5))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ve1qjcRe1N7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_demo_df[age_groups].mean().plot.bar(logy=True, figsize=(16,8))\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Mean Population')\n",
        "plt.title('Mean Population by Age Group in City Demographics')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-ExLJbcroQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUTlcM64l91f"
      },
      "source": [
        "###County Demographics Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MA9UiHVmB2m"
      },
      "outputs": [],
      "source": [
        "county_numerical_cols = list(county_demo_df.select_dtypes(include=[\"int64\", \"float64\"]).columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix\n",
        "county_corr = county_demo_df[county_numerical_cols].corr()"
      ],
      "metadata": {
        "id": "W8fJKoip7jzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(160, 80))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(county_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9_Q5pIu5IrO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df.plot.scatter(x='Under 5 years', y='85 years and over', logx=True, logy=True, s=40, alpha=.8, figsize=(16,8))"
      ],
      "metadata": {
        "id": "_ZOV2qpz796k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "county_demo_df[age_groups].mean().plot.bar(figsize=(20,5))\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Mean Population')\n",
        "plt.title('Mean Population by Age Group in City Demographics')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l18V6BHn8WZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vinl2fh1mEf2"
      },
      "source": [
        "##Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrV7bfizmKKG"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op0QoPC9b2fP"
      },
      "outputs": [],
      "source": [
        "#convert the categorical columns to numerical ones and store the modified df as a new df for correlation analysis\n",
        "categorical_to_int_df = merged_df\n",
        "# Select only the numerical columns\n",
        "numerical_cols = categorical_to_int_df.select_dtypes(include=['int64', 'float64', np.number]).drop(labels=[\"incident_id\"], axis=1)\n",
        "# numerical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHN8Dby4dvXw"
      },
      "outputs": [],
      "source": [
        "# Create a correlation matrix\n",
        "merged_corr = merged_df[int_columns].dropna().corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(160, 80))\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(county_corr, annot=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0nuhpdPeyRp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyei2nuHfjgI"
      },
      "outputs": [],
      "source": [
        "# merged_df[\"color\"] = merged_df[\"region_name\"].map({\n",
        "#     \"Midwest\": \"red\",\n",
        "#     \"West\": \"yellow\",\n",
        "#     \"Northeast\": \"green\",\n",
        "#     \"South\": \"blue\"\n",
        "# })\n",
        "# merged_df[\"size\"] = merged_df.groupby([\"incident_month\", \"region_name\"]).count().reset_index()[\"incident_id\"]\n",
        "\n",
        "# #remove the max row limit for altair\n",
        "# alt.data_transformers.disable_max_rows()\n",
        "\n",
        "# alt.Chart(merged_df.dropna()).mark_circle().encode(\n",
        "#     x=\"total_offender_count\",\n",
        "#     y=\"total_individual_victims\",\n",
        "#     color=alt.Color(\"color\", scale=None),\n",
        "#     size=\"size\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4_xXvj5uXc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oYwpwcA8aqsI",
        "zouCbvk6_ccD",
        "8hlE0Oe8bUbd",
        "yYKBzufDuKl5",
        "8PgKZ6LO0rur",
        "_jVRTkrMofxu",
        "HiZ4Oczk1F9_",
        "B6hhocTJ_N5q",
        "NXLlSu1U_T9I",
        "k9ASyJ510639",
        "eA_zceh1oTPM",
        "jzeM5TSsoKeU",
        "7s_ph3jfiVxi",
        "Fm5xNM5iePPX",
        "Eh9zD204ltAu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}